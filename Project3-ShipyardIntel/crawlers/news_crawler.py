"""
news_crawler.py - ì¡°ì„ ì—… ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (v2 - 3ì¹´í…Œê³ ë¦¬ êµ¬ì¡°)

category:
  ko_company  â†’ í•œêµ­ ì¡°ì„  3ì‚¬ (í•œí™”ì˜¤ì…˜, ì‚¼ì„±ì¤‘ê³µì—…, HDí˜„ëŒ€ì¤‘ê³µì—…Â·ë¯¸í¬Â·ì‚¼í˜¸Â·KSOE)
  ko_policy   â†’ ì¡°ì„  ì •ì±… (í•´ì–‘ìˆ˜ì‚°ë¶€, IMO, í•œêµ­ ì •ë¶€ ì¡°ì„ ì‚°ì—…)
  international â†’ MASGAÂ·êµ­ì œ ì¡°ì„  ë‰´ìŠ¤ (gCaptain, Marine Log, Maritime Executive)
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Optional
import sys, os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from db.database import upsert_news

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    )
}

def _get(url: str, timeout: int = 15) -> Optional[BeautifulSoup]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return BeautifulSoup(r.text, "lxml")
    except Exception as e:
        print(f"[NEWS] fetch error ({url}): {e}")
        return None

def _extract(soup: BeautifulSoup, selectors: list, base_url: str, source: str, lang: str, category: str, limit: int = 20) -> list:
    """ê³µí†µ ì¶”ì¶œ ë¡œì§ - í—¤ë“œë¼ì¸ ìœ„ì£¼ (summary ìƒëµìœ¼ë¡œ ê²½ëŸ‰í™”)"""
    items = []
    for sel in selectors:
        articles = soup.select(sel)
        if not articles:
            continue
        for art in articles[:limit]:
            title_el = art.select_one("h1,h2,h3,h4,.title,.entry-title,.headline,.subject")
            link_el = art.select_one("a[href]")
            date_el = art.select_one("time,.date,.regdate,.entry-date,.published")
            if not title_el or not link_el:
                # title_elì´ a íƒœê·¸ì¼ ìˆ˜ë„
                if art.name == "a" and art.get_text(strip=True):
                    link_el = art
                    title_el = art
                else:
                    continue
            href = link_el.get("href", "")
            if not href.startswith("http"):
                href = base_url.rstrip("/") + "/" + href.lstrip("/")
            title = title_el.get_text(strip=True)
            if not title or len(title) < 5:
                continue
            items.append({
                "title": title,
                "summary": "",  # í—¤ë“œë¼ì¸ ìœ„ì£¼ë¡œ summary ìƒëµ
                "url": href,
                "source": source,
                "language": lang,
                "category": category,
                "published_at": date_el.get_text(strip=True)[:10] if date_el else datetime.now().strftime("%Y-%m-%d"),
            })
        if items:
            break
    return items


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¹´í…Œê³ ë¦¬ 1: ğŸ‡°ğŸ‡· ko_company - ì¡°ì„  3ì‚¬ ë‰´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def crawl_ko_company_gnews() -> list:
    """Google News RSS - í•œêµ­ ì¡°ì„  3ì‚¬ ë‰´ìŠ¤ (í•œí™”ì˜¤ì…˜/ì‚¼ì„±ì¤‘ê³µì—…/HDí˜„ëŒ€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤)"""
    import xml.etree.ElementTree as ET
    items = []
    queries = [
        ("í•œí™”ì˜¤ì…˜", "í•œí™”ì˜¤ì…˜"),
        ("ì‚¼ì„±ì¤‘ê³µì—…", "ì‚¼ì„±ì¤‘ê³µì—…"),
        ("HDí˜„ëŒ€ì¤‘ê³µì—… OR HDí•œêµ­ì¡°ì„ í•´ì–‘", "HDí˜„ëŒ€"),
        ("í˜„ëŒ€ë¯¸í¬ì¡°ì„  OR í˜„ëŒ€ì‚¼í˜¸ì¤‘ê³µì—…", "í˜„ëŒ€ë¯¸í¬Â·ì‚¼í˜¸"),
    ]
    from urllib.parse import quote
    for q, label in queries:
        url = f"https://news.google.com/rss/search?q={quote(q)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            r = requests.get(url, headers=HEADERS, timeout=12)
            root = ET.fromstring(r.content)
            for item in root.findall(".//item")[:8]:
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("pubDate")
                if title_el is None or link_el is None:
                    continue
                title = title_el.text or ""
                # ê´‘ê³ /ìŠ¤í¬ì¸  ë“± ë¹„ê´€ë ¨ ì œê±°
                if any(kw in title for kw in ["ê²½ê¸°", "ì•¼êµ¬", "ì¶•êµ¬", "ì£¼ê°€", "ì¦ì‹œ"]):
                    continue
                pub = (pub_el.text or "")[:16] if pub_el is not None else datetime.now().strftime("%Y-%m-%d")
                items.append({
                    "title": f"[{label}] {title}",
                    "summary": "",
                    "url": link_el.text or "",
                    "source": f"{label} (êµ¬ê¸€ë‰´ìŠ¤)",
                    "language": "ko",
                    "category": "ko_company",
                    "published_at": pub[:10],
                })
        except Exception as e:
            print(f"[NEWS] Google News RSS ({label}): {e}")
    print(f"[NEWS] Google News ì¡°ì„ 3ì‚¬: {len(items)} articles")
    return items



def crawl_maritime_korea() -> list:
    """í•´ì–‘í•œêµ­ - êµ­ë‚´ ì¡°ì„  ì „ë¬¸ì§€ (ko_company)"""
    items = []
    for url in ["http://www.monthlymaritimekorea.com/news/articleList.html", "http://www.monthlymaritimekorea.com/"]:
        soup = _get(url)
        if not soup:
            continue
        articles = soup.select(".article-list li, article, .news-list li")
        for art in articles[:20]:
            title_el = art.select_one("a, h3, h4, .title")
            link_el = art.select_one("a[href]")
            date_el = art.select_one(".date, time")
            if not title_el or not link_el:
                continue
            href = link_el.get("href", "")
            if not href.startswith("http"):
                href = "http://www.monthlymaritimekorea.com" + href
            title = title_el.get_text(strip=True)
            if len(title) < 5:
                continue
            items.append({
                "title": title, "summary": "", "url": href,
                "source": "í•´ì–‘í•œêµ­", "language": "ko",
                "category": "ko_company",
                "published_at": date_el.get_text(strip=True)[:10] if date_el else datetime.now().strftime("%Y-%m-%d"),
            })
        if items:
            break
    print(f"[NEWS] í•´ì–‘í•œêµ­: {len(items)} articles")
    return items


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¹´í…Œê³ ë¦¬ 2: ğŸ“‹ ko_policy - ì¡°ì„  ì •ì±… ë‰´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def crawl_mof_policy() -> list:
    """í•´ì–‘ìˆ˜ì‚°ë¶€ ë³´ë„ìë£Œ - ì¡°ì„ ì‚°ì—… ì •ì±…"""
    items = []
    for url in [
        "https://www.mof.go.kr/doc/ko/selectDoc.do?bbsId=PRESS_RELEASE&menuSeq=971",
        "https://www.mof.go.kr/doc/ko/selectDoc.do?bbsId=PRESS_RELEASE",
    ]:
        soup = _get(url)
        if not soup:
            continue
        rows = soup.select("table tbody tr, .board-list li, article")
        count = 0
        for row in rows[:30]:
            title_el = row.select_one("a, .title, h3, td.subject")
            link_el = row.select_one("a[href]")
            date_el = row.select_one("td.date, .date, time")
            if not title_el or not link_el:
                continue
            title = title_el.get_text(strip=True)
            # ì¡°ì„  ê´€ë ¨ í•„í„°
            if not any(kw in title for kw in ["ì¡°ì„ ", "ì„ ë°•", "í•´ìš´", "LNG", "í•­ë§Œ", "í•´ì–‘"]):
                continue
            href = link_el.get("href", "")
            if not href.startswith("http"):
                href = "https://www.mof.go.kr" + href
            items.append({
                "title": f"[í•´ì–‘ìˆ˜ì‚°ë¶€] {title}",
                "summary": "", "url": href,
                "source": "í•´ì–‘ìˆ˜ì‚°ë¶€", "language": "ko",
                "category": "ko_policy",
                "published_at": date_el.get_text(strip=True)[:10] if date_el else datetime.now().strftime("%Y-%m-%d"),
            })
            count += 1
            if count >= 10:
                break
        if items:
            break
    print(f"[NEWS] í•´ì–‘ìˆ˜ì‚°ë¶€: {len(items)} articles")
    return items


def crawl_ko_policy_gnews() -> list:
    """Google News RSS - ì¡°ì„  ì •ì±… ë‰´ìŠ¤ (í•´ì–‘ìˆ˜ì‚°ë¶€/ì‚°ì—…ë¶€ ì ‘ê·¼ ë¶ˆê°€ ì‹œ í´ë°±)"""
    import xml.etree.ElementTree as ET
    items = []
    queries = [
        ("ì¡°ì„ ì—… ì •ì±… OR ì¡°ì„  ì§€ì› OR ì„ ë°• ìˆ˜ì¶œ ì§€ì›", "ì¡°ì„ ì •ì±…"),
        ("LNG ì„ ë°• ì •ì±… OR í•´ì–‘ ì •ì±… OR IMO ì¡°ì„ ", "í•´ì–‘ì •ì±…"),
        ("MASGA OR Make American Shipbuilding OR US shipbuilding policy", "MASGAì •ì±…"),
    ]
    from urllib.parse import quote
    for q, label in queries:
        url = f"https://news.google.com/rss/search?q={quote(q)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            r = requests.get(url, headers=HEADERS, timeout=12)
            root = ET.fromstring(r.content)
            for item in root.findall(".//item")[:6]:
                title_el = item.find("title")
                link_el = item.find("link")
                pub_el = item.find("pubDate")
                if title_el is None or link_el is None:
                    continue
                title = title_el.text or ""
                pub = (pub_el.text or "")[:16] if pub_el is not None else datetime.now().strftime("%Y-%m-%d")
                items.append({
                    "title": f"[{label}] {title}",
                    "summary": "",
                    "url": link_el.text or "",
                    "source": f"{label} (êµ¬ê¸€ë‰´ìŠ¤)",
                    "language": "ko",
                    "category": "ko_policy",
                    "published_at": pub[:10],
                })
        except Exception as e:
            print(f"[NEWS] Google News RSS Policy ({label}): {e}")
    print(f"[NEWS] Google News ì •ì±…: {len(items)} articles")
    return items



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì¹´í…Œê³ ë¦¬ 3: ğŸŒ international - MASGA/êµ­ì œ ë‰´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def crawl_gcaptain() -> list:
    """gCaptain - MASGA í¬í•¨ êµ­ì œ ì¡°ì„ /í•´ìš´ ë‰´ìŠ¤"""
    items = []
    for url in [
        "https://gcaptain.com/category/shipbuilding/",
        "https://gcaptain.com/",
    ]:
        soup = _get(url)
        if not soup:
            continue
        articles = soup.select("article, .post, .entry")
        for art in articles[:25]:
            title_el = art.select_one("h2, h3, .entry-title, .post-title")
            link_el = art.select_one("a[href]")
            date_el = art.select_one("time, .entry-date, .published")
            if not title_el or not link_el:
                continue
            href = link_el.get("href", "")
            if not href.startswith("http"):
                href = "https://gcaptain.com" + href
            title = title_el.get_text(strip=True)
            if not title or len(title) < 10:
                continue
            # MASGA íƒœê·¸ ìë™ ë¶€ì—¬
            is_masga = any(kw in title.lower() for kw in ["american", "usa", "u.s.", "trump", "masga", "executive order", "jones act"])
            items.append({
                "title": ("ğŸ‡ºğŸ‡¸ [MASGA] " if is_masga else "") + title,
                "summary": "", "url": href,
                "source": "gCaptain", "language": "en",
                "category": "international",
                "published_at": date_el.get("datetime", "")[:10] if date_el else datetime.now().strftime("%Y-%m-%d"),
            })
        if items:
            break
    print(f"[NEWS] gCaptain: {len(items)} articles")
    return items


def crawl_marine_log() -> list:
    """Marine Log - ë¯¸êµ­ ì¡°ì„ /í•´ìš´ ì—…ê³„ì§€"""
    items = []
    for url in ["https://www.marinelog.com/news/", "https://www.marinelog.com/"]:
        soup = _get(url)
        if not soup:
            continue
        articles = soup.select("article, .post, .entry")
        for art in articles[:20]:
            title_el = art.select_one("h2, h3, .entry-title")
            link_el = art.select_one("a[href]")
            date_el = art.select_one("time, .entry-date")
            if not title_el or not link_el:
                continue
            href = link_el.get("href", "")
            title = title_el.get_text(strip=True)
            if not title or len(title) < 10:
                continue
            is_masga = any(kw in title.lower() for kw in ["american", "u.s.", "trump", "shipbuilding act", "masga"])
            items.append({
                "title": ("ğŸ‡ºğŸ‡¸ [MASGA] " if is_masga else "") + title,
                "summary": "", "url": href,
                "source": "Marine Log", "language": "en",
                "category": "international",
                "published_at": date_el.get("datetime", "")[:10] if date_el else datetime.now().strftime("%Y-%m-%d"),
            })
        if items:
            break
    print(f"[NEWS] Marine Log: {len(items)} articles")
    return items


def crawl_maritime_executive() -> list:
    """Maritime Executive - êµ­ì œ ì¡°ì„ /í•´ìš´"""
    items = []
    for url in ["https://maritime-executive.com/article", "https://maritime-executive.com"]:
        soup = _get(url)
        if not soup:
            continue
        articles = soup.select("article, .card, .story-item")
        for art in articles[:20]:
            title_el = art.select_one("h2, h3, .card-title, .entry-title")
            link_el = art.select_one("a[href]")
            if not title_el or not link_el:
                continue
            href = link_el.get("href", "")
            if not href.startswith("http"):
                href = "https://maritime-executive.com" + href
            title = title_el.get_text(strip=True)
            if not title or len(title) < 10:
                continue
            items.append({
                "title": title, "summary": "", "url": href,
                "source": "Maritime Executive", "language": "en",
                "category": "international",
                "published_at": datetime.now().strftime("%Y-%m-%d"),
            })
        if items:
            break
    print(f"[NEWS] Maritime Executive: {len(items)} articles")
    return items


def run_all() -> int:
    all_items = []
    # ì¡°ì„  3ì‚¬ (Google News RSS - ê°€ì¥ ì•ˆì •ì )
    all_items += crawl_ko_company_gnews()
    all_items += crawl_maritime_korea()   # í•´ì–‘í•œêµ­ (ì‘ë™ í™•ì¸)
    # ì •ì±… (Google News RSS í´ë°±)
    all_items += crawl_mof_policy()       # í•´ìˆ˜ë¶€ ì§ì ‘ ì‹œë„
    all_items += crawl_ko_policy_gnews()  # Google News í´ë°±
    # êµ­ì œ (MASGA í¬í•¨)
    all_items += crawl_gcaptain()
    all_items += crawl_marine_log()
    all_items += crawl_maritime_executive()

    saved = upsert_news(all_items)
    print(f"[NEWS] Total new articles saved: {saved}/{len(all_items)}")
    return saved


if __name__ == "__main__":
    from db.database import init_db
    init_db()
    run_all()
